{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set TIME FREQUENCY\n",
    "#options are: 'Monthly', 'Weekly', 'Quarterly'\n",
    "Time_freq = 'Monthly'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Imports & Settings\n",
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rv_standard_functions_git import *\n",
    "from evaluate import *\n",
    "from aws import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_settings()\n",
    "from math import exp,log\n",
    "import boto3\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for which countries to run price index\n",
    "countries = []\n",
    "# Select the S3 parameters\n",
    "bucket = 'lpd.data.sci.intermediate.tst'\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Select whether or not to use a specific fueltype cars\n",
    "use_specific_fueltype = True\n",
    "fueltype = 'Diesel'\n",
    "\n",
    "export_date = '20200607'\n",
    "\n",
    "begin_date = '2013-01-31'\n",
    "end_date = '2019-12-31'\n",
    "train_start = pd.to_datetime('2013-01-31').date()\n",
    "train_end = pd.to_datetime('2019-12-31').date()\n",
    "\n",
    "model_ids= {\"Fill in model ids\"}\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to clean the names of the car makes\n",
    "def clean_make_names(data):\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-3:]!=' EL' else x[:-3])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-4:]!=' T.T' else x[:-4])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-3:]!=' TT' else x[:-3])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!=' T.T.' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-9:]!=' INDUSTR.' else x[:-9])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-8:]!=' INDUST.' else x[:-8])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-11:]!=' BEDR.WAGEN' else x[:-11])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-4:]!=' 2+2' else x[:-4])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!=' IND.' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!=' V.I.' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-4:]!='-NFZ' else x[:-4])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-7:]!=' TRUCKS' else x[:-7])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!='-BENZ' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!=' BENZ' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-5:]!=' Benz' else x[:-5])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-7:]!=' TRUCKS' else x[:-7])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x[-3:]!='-VI' else x[:-3])\n",
    "    data['make'] = data['make'].apply(lambda x: x if x.isupper()==True else x.upper())\n",
    "    data['model_cons'] = np.where((data.model_cons == 'PASSAT CC'),'PASSAT',data.model_cons)\n",
    "    data['make'] = np.where((data.make == 'VW'),'VOLKSWAGEN',data.make)\n",
    "    data['make'] = np.where((data.make == 'Audi'),'AUDI',data.make)\n",
    "\n",
    "    # Create make_model variable\n",
    "    data['make_model'] = data['make'] + '_' + data['model_cons']\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create graphs Time Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred, print_ = False):\n",
    "    \"\"\"\n",
    "    Function to calculate the mean absolute percentage errors.\n",
    "    Takes lists as input\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    if print_:\n",
    "        print(np.abs((y_true - y_pred) / y_true))\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_graph(data, top10 = False, graph_name = '', frequency = 'Monthly'):\n",
    "    \"\"\"\n",
    "    Function that applies the hedonic time dummy method for creating an index.\n",
    "    \"\"\"\n",
    "    \n",
    "    import datetime    \n",
    "    \n",
    "    #Add a new variable for the period of the sale date\n",
    "    if frequency == 'Monthly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('M'))\n",
    "    elif frequency == 'Quarterly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('Q'))\n",
    "    elif frequency == 'Weekly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('W'))\n",
    "    \n",
    "    #Give all periods in benchmark year the same 'date'\n",
    "    unique_ts = data.freq_ts.unique()\n",
    "    unique_ts.sort()\n",
    "    unique_ts_1 = [x for x in unique_ts if x.to_timestamp().date() <= datetime.date(2013, 12, 31)]\n",
    "    unique_ts_2 = [x for x in unique_ts if x.to_timestamp().date() > datetime.date(2013, 12, 31)]\n",
    "        \n",
    "    if top10:\n",
    "        data = data.loc[data.model_cons.isin(data.model_cons.value_counts().index[:10])]\n",
    "\n",
    "    data.loc[data.freq_ts.isin(unique_ts_1), 'freq_ts'] = None\n",
    "    \n",
    "    \n",
    "    #Only look at a specifif fueltype if necessary\n",
    "    if use_specific_fueltype:\n",
    "        data = data.loc[data['fueltype'] == fueltype]\n",
    "\n",
    "    # Splitting the data will aslo dummify the period variable\n",
    "    data, train, test, X_train, X_test, y_train, y_test, weights = split_preprocess_data(data, params['categorical_vars'], \n",
    "                                                                                     params['continuous_vars'])\n",
    "\n",
    "    # Use log of sellprice as a target\n",
    "    y_train = np.log(y_train.values)\n",
    "\n",
    "    # Train a linear model\n",
    "    model_linear = train_linear(X_train, y_train)\n",
    "\n",
    "    # Store the variable coefficietns in a dictionary\n",
    "    coefficients = model_linear.params.to_dict()\n",
    "\n",
    "\n",
    "\n",
    "    ts_coefficients = []\n",
    "\n",
    "    # Get the the coefficients from the trained model for 2020\n",
    "    for k,v in coefficients.items():\n",
    "        if k[:7]=='freq_ts':\n",
    "            ts_coefficients.append(v)\n",
    "\n",
    "        # Take the exponent of the period coeficients to trace the development\n",
    "    ts_coefficients = np.exp(np.array(ts_coefficients ,dtype=np.float128))\n",
    "    \n",
    "    from datetime import datetime\n",
    "\n",
    "\n",
    "    # Creating a final dataframe and plotting the results\n",
    "    dataset = pd.DataFrame({'Period': unique_ts_2, '{}'.format(graph_name): ts_coefficients})\n",
    "    dataset = dataset.set_index('Period')\n",
    "    return dataset, model_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hed_Graph(data, frequency = 'Monthly'):\n",
    "    \n",
    "    \n",
    "    factors_x = [ \"fill in factors here\"]\n",
    "\n",
    "    factors_y = ['sellprice']\n",
    "    \n",
    "    if frequency == 'Monthly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('M'))\n",
    "    elif frequency == 'Quarterly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('Q'))\n",
    "    elif frequency == 'Weekly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('W'))\n",
    "        \n",
    "    data_mod = data.loc[data.model_cons.isin(data.model_cons.value_counts().index[:10])]\n",
    "\n",
    "    dates_ts = data_mod.freq_ts.unique().tolist()\n",
    "    dates_ts.sort()\n",
    "    dates_ts_1 = [x for x in dates_ts if x.to_timestamp().date() <= datetime.date(2013, 12, 31)]\n",
    "    dates_ts_2 = [x for x in dates_ts if x.to_timestamp().date() > datetime.date(2013, 12, 31)]\n",
    "    \n",
    "    selection = pd.DataFrame()\n",
    "\n",
    "    for i in data_mod.model_encoded.unique():\n",
    "        data_sub = data_mod.loc[data_mod.model_encoded == i]\n",
    "        selection = selection.append(data_sub.sample(20))\n",
    "        \n",
    "    params_bayes = dict(zip(['colsample_bytree', 'learning_rate', 'max_depth', 'alpha', 'gamma', 'min_child_weight'],\n",
    "                       [0.6963, 0.09918, 3.475, 4.996, 0.253, 1.685]))\n",
    "    \n",
    "    total_xgreg = []\n",
    "    models_fin = []\n",
    "    models_use = []\n",
    "    x_use = []\n",
    "    index_vals = []\n",
    "    bench = []\n",
    "    errors_total = []\n",
    "    for i in dates_ts_1:\n",
    "        try:\n",
    "            data_cur = data_mod.loc[data_mod.freq_ts ==i]\n",
    "            data_cur = data_cur[factors_x + factors_y]\n",
    "            data_cur = data_cur.dropna()\n",
    "            X, y = data_cur[factors_x], data_cur[factors_y]\n",
    "            xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = params_bayes['colsample_bytree'], learning_rate = params_bayes['learning_rate'],\n",
    "                        max_depth = int(params_bayes['max_depth']), alpha = params_bayes['alpha'], gamma = params_bayes['gamma'], min_child_weight = int(params_bayes['min_child_weight']),\n",
    "                                      n_estimators = 300, early_stopping_rounds=20)\n",
    "\n",
    "\n",
    "            xg_reg.fit(X,y)\n",
    "            models_fin.append(xg_reg)\n",
    "\n",
    "            preds = xg_reg.predict(X)\n",
    "            #print('MAPE xgboost:', mean_absolute_percentage_error(data_cur.sellprice.tolist(), preds.tolist()))\n",
    "\n",
    "            selection_cur = selection[factors_x]\n",
    "\n",
    "            preds_new = xg_reg.predict(selection_cur)\n",
    "            preds_new = np.mean(preds_new)\n",
    "            bench.append(preds_new)\n",
    "        except:\n",
    "            models_fin.append(None)\n",
    "            bench.append(None)\n",
    "\n",
    "    for i in dates_ts_2:\n",
    "        try:\n",
    "            data_cur = data_mod.loc[data_mod.freq_ts ==i]\n",
    "            data_cur = data_cur[factors_x + factors_y]\n",
    "            data_cur = data_cur.dropna()\n",
    "                \n",
    "            X, y = data_cur[factors_x], data_cur[factors_y]\n",
    "            xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = params_bayes['colsample_bytree'], learning_rate = params_bayes['learning_rate'],\n",
    "                        max_depth = int(params_bayes['max_depth']), alpha = params_bayes['alpha'], gamma = params_bayes['gamma'], min_child_weight = int(params_bayes['min_child_weight']),\n",
    "                                      n_estimators = 300, early_stopping_rounds=20)\n",
    "\n",
    "\n",
    "            xg_reg.fit(X,y)\n",
    "            models_fin.append(xg_reg)\n",
    "            if len(data_cur) > 800:\n",
    "                models_use.append(xg_reg)\n",
    "                x_use.append(X)\n",
    "            preds = xg_reg.predict(X)\n",
    "            errors_total.append(mean_absolute_percentage_error(data_cur.sellprice.tolist(), preds.tolist()))\n",
    "            selection_cur = selection[factors_x]\n",
    "\n",
    "            preds_new = xg_reg.predict(selection_cur)\n",
    "            preds_new = np.mean(preds_new)\n",
    "\n",
    "            index_vals.append(preds_new/np.mean(bench))\n",
    "        except:\n",
    "            index_vals.append(None)\n",
    "        \n",
    "    #print(models_fin)\n",
    "    return pd.Series(index = dates_ts_2, data =index_vals), models_use, x_use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def Covid_Graph(data, frequency = 'Monthly'):\n",
    "    \n",
    "    data['selldate2'] = pd.to_datetime(data.selldate).dt.date\n",
    "    \n",
    "    if frequency == 'Monthly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('M'))\n",
    "    elif frequency == 'Quarterly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('Q'))\n",
    "    elif frequency == 'Weekly':\n",
    "        data['freq_ts'] = list(data['selldate'].dt.to_period('W'))\n",
    "    \n",
    "\n",
    "    num_make_models = 10\n",
    "    bandwidth_var1 = 0.3\n",
    "    bandwidth_var2 = 0.3\n",
    "    bandwidth_var3 = 0.3\n",
    "\n",
    "    bench_year = 2013\n",
    "    \n",
    "    dates_ts = data.freq_ts.unique().tolist()\n",
    "    dates_ts.sort()\n",
    "    dates_ts_1 = [x for x in dates_ts if x.to_timestamp().date() <= datetime.date(2013, 12, 31)]\n",
    "    dates_ts_2 = [x for x in dates_ts if x.to_timestamp().date() > datetime.date(2013, 12, 31)]\n",
    "\n",
    "    \n",
    "    #Find the top 10 model_cons based on the benchmark year\n",
    "    #benchmark_year = data.loc[(data.selldate2 >= datetime.date(bench_year, 1, 1)) & (data.selldate2 < datetime.date(bench_year + 1, 1, 1))]\n",
    "    \n",
    "    benchmark_year = data.loc[data.freq_ts.isin(dates_ts_1)]\n",
    "    top10_cars = data.model_cons.value_counts().index[:10].tolist()\n",
    "    benchmark_year = benchmark_year.loc[benchmark_year.model_cons.isin(top10_cars)]\n",
    "\n",
    "    #The full dataset for the top10cars\n",
    "    data_CBPI = data.loc[data.model_cons.isin(top10_cars)]\n",
    "\n",
    "    data_filtered = pd.DataFrame()\n",
    "    medians_all = pd.DataFrame()\n",
    "\n",
    "    for mm in top10_cars:\n",
    "        data_sub = data_CBPI[(data.model_cons == mm)]\n",
    "\n",
    "        median_var1 = np.median(data_sub.var1)\n",
    "        median_var2 = np.median(data_sub.var2)\n",
    "        median_var3 = np.median(data_sub.var3)\n",
    "\n",
    "        # Filter observations outside of median var1 bandwidth\n",
    "        data_sub = data_sub[(data_sub.var1 > ((1 - bandwidth_var1) * median_var1))]\n",
    "        data_sub = data_sub[(data_sub.var1 < ((1 + bandwidth_var1) * median_var1))]\n",
    "    #         print('median var1: ' + str(median_var1))\n",
    "\n",
    "        # Filter observations outside of median var1 bandwidth\n",
    "        data_sub = data_sub[(data_sub.var2 > ((1 - bandwidth_var2) * median_var2))]\n",
    "        data_sub = data_sub[(data_sub.var2 < ((1 + bandwidth_var2) * median_var2))]\n",
    "    #         print('median var2: ' + str(median_var2))\n",
    "\n",
    "        # Filter observations outside of median var3 price bandwidth\n",
    "        data_sub = data_sub[(data_sub.var3 > ((1 - bandwidth_var3) * median_var3))]\n",
    "        data_sub = data_sub[(data_sub.var3 < ((1 + bandwidth_var3) * median_var3))]\n",
    "    #         print('median var3: ' + str(median_var3))\n",
    "\n",
    "        # Merge data\n",
    "        data_filtered = pd.concat([data_filtered, data_sub])\n",
    "\n",
    "        medians_mm = [median_var1, median_var2, median_var3]\n",
    "        medians_all = pd.concat([medians_all, pd.DataFrame(medians_mm)], axis=1)\n",
    "\n",
    "    medians_all.index = ['median_var1', 'median_var2', 'median_var3']\n",
    "    medians_all.columns = top10_cars\n",
    "\n",
    "    data_cur = data_filtered.copy()\n",
    "    data_cur['week'] = data_cur['selldate'].apply(lambda x: int(x.strftime(\"%V\")))\n",
    "    data_cur['year'] = data_cur['selldate'].apply(lambda x: int(x.year))\n",
    "    data_cur['month'] = data_cur['selldate'].apply(lambda x: int(x. month))\n",
    "    \n",
    "    # Get baseline per make_model, first average out per period, and then average over the periods\n",
    "    data_bench = data_cur[(data_cur.year == bench_year)]\n",
    "    grouped = data_bench.groupby(['freq_ts', 'model_cons'])['sellprice'].agg('mean')\n",
    "    baseline = np.mean(grouped.unstack())\n",
    "\n",
    "    # Get price index per make_model\n",
    "    data_new = data_cur[data_cur.freq_ts.isin(dates_ts_2)]\n",
    "    grouped = data_new.groupby(['freq_ts', 'model_cons'])['sellprice'].agg('mean')\n",
    "    price_index_mm = grouped.unstack()/baseline\n",
    "    price_index = price_index_mm.mean(axis=1) \n",
    "    \n",
    "    return price_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Select the model id from the dictionary provided earlier\n",
    "    model_id = int(model_ids[country])\n",
    "\n",
    "    # Get the params we use production for the XGB model\n",
    "    try:\n",
    "        params = get_from_s3(filename='params', \n",
    "                             project=country, \n",
    "                             model_id=model_id)\n",
    "    except:\n",
    "        try:\n",
    "            params = get_from_s3(filename='parameters', \n",
    "                                 project=country, \n",
    "                                 model_id=model_id)\n",
    "        except:\n",
    "            print(\"No parameters found for {0}\".format(country))\n",
    "\n",
    "    # We set these params to False in order to use clean data\n",
    "    params['train_start'] = train_start\n",
    "    params['train_end'] = train_end\n",
    "    params['car_model'] = 'all'\n",
    "    params['rebase'] = False\n",
    "    params['bulking_correction'] = False\n",
    "    params['bulking_train_out'] = False\n",
    "    \n",
    "    # Get the data \n",
    "    trainset = get_data_trainset(start_date_trainset=params['train_start'],\n",
    "                                 end_date_trainset=params['train_end'], \n",
    "                                 car_model=params['car_model'],\n",
    "                                 country=country,\n",
    "                                 rebase = params['rebase'],\n",
    "                                 bulk_train_out = params['bulk_train_out'],\n",
    "                                 query_path = './sql_query_train_RVdb1.sql',\n",
    "                                 export_date = export_date)\n",
    "\n",
    "    # Make correction for car names\n",
    "    trainset = clean_make_names(trainset)\n",
    "\n",
    "    # Preprocessing\n",
    "    model_id = generate_id()\n",
    "    data = preprocess_data(trainset=trainset,\n",
    "                           testset=pd.DataFrame(),\n",
    "                           country=country,\n",
    "                           model_id=model_id)\n",
    "\n",
    "    # Remove duplicates\n",
    "    data['duplicated'] = data.duplicated('lkey_vehicle')\n",
    "    data = data[(data['lkey_vehicle'].isnull()) | (data['lkey_vehicle'].isna()) | (data['duplicated'] == False)]\n",
    "\n",
    "    #Only look at train data\n",
    "    data = data.loc[data.train_test_active_flag == 'train']\n",
    "    if use_specific_fueltype:\n",
    "        data = data.loc[data.fueltype == fueltype]    \n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    data_copy.model_encoded = data_copy.model_encoded.map(lambda x: str(x)) #\n",
    "    params['categorical_vars'].append('freq_ts') \n",
    "    \n",
    "    params['categorical_vars'].append('model_encoded') #\n",
    "    params['continuous_vars'].remove('model_encoded') #\n",
    "    \n",
    "\n",
    "\n",
    "    data = data_copy.copy()\n",
    "    #dataset = TD_graph(data, top10 = False, graph_name = 'Time Dummy (All)')\n",
    "    dataset, model_linear = TD_graph(data, top10 = False, graph_name = 'Time Dummy method', frequency = Time_freq)\n",
    "    \n",
    "    data = data_copy.copy()\n",
    "    #dataset = pd.concat([dataset, TD_graph(data, top10=True, graph_name = 'Time Dummy (top10 make_models)')], axis = 1)\n",
    "    #dataset['Hedonic XGBoost'] = Hed_Graph(data)\n",
    "    imp_list, models_fin, x_fin = Hed_Graph(data, frequency=Time_freq)\n",
    "    #dataset['Imputation method'], models_fin = Hed_Graph(data, frequency = Time_freq)\n",
    "    dataset['Imputation method'] = imp_list\n",
    "    \n",
    "    data = data_copy.copy()\n",
    "    #dataset['Covid Index'] = Covid_Graph(data)\n",
    "    dataset['Average Bucket Method'] = Covid_Graph(data, frequency = Time_freq)\n",
    "    \n",
    "    display(dataset.plot(figsize=(15, 7)))\n",
    "    plt.savefig('plot_{}_index.png'.format(country), dpi=200)\n",
    "    dataset.to_csv('dataset_{}_index_freq_{}.csv'.format(country, Time_freq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
